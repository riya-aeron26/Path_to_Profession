{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T13:39:23.641111Z",
     "iopub.status.busy": "2023-11-02T13:39:23.640357Z",
     "iopub.status.idle": "2023-11-02T14:04:03.972442Z",
     "shell.execute_reply": "2023-11-02T14:04:03.971328Z",
     "shell.execute_reply.started": "2023-11-02T13:39:23.641075Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0bf0f076534af289d0cc04ed3b4015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44384d4022047c98f068aab0084db5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7037b40ff614378bf7183f23d8a5d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb347c68319448088ba9b1b2195388b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3680dc14f384121b9c4d55ff4184ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20231102_134158-hclxe4qh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/no_namee/huggingface/runs/hclxe4qh' target=\"_blank\">polar-firefly-3</a></strong> to <a href='https://wandb.ai/no_namee/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/no_namee/huggingface' target=\"_blank\">https://wandb.ai/no_namee/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/no_namee/huggingface/runs/hclxe4qh' target=\"_blank\">https://wandb.ai/no_namee/huggingface/runs/hclxe4qh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4351' max='4351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4351/4351 21:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.235300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.926900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.795700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.729200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.698300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.675700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.652800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.602600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define a padding token for the tokenizer (e.g., [PAD])\n",
    "# You can use a different token if you prefer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load and preprocess your dataset (job descriptions and job titles)\n",
    "dataset = pd.read_csv(\"/kaggle/input/gpttrain/x.csv\")\n",
    "\n",
    "# Handle NaN values in the \"jobdescription\" column\n",
    "dataset[\"jobdescription\"].fillna(\"\", inplace=True)\n",
    "\n",
    "# Tokenize the dataset\n",
    "text_data = dataset[\"jobdescription\"].tolist()\n",
    "\n",
    "# Create a list of texts with a special separator token [SEP]\n",
    "texts = [\"[SEP] \" + text for text in text_data]\n",
    "\n",
    "# Prepare the training dataset\n",
    "with open(\"your_training_data.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(texts))\n",
    "\n",
    "text_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"your_training_data.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine-tuned-model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Create a Trainer instance for fine-tuning\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=text_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./fine-tuned-model\")\n",
    "\n",
    "# Load the fine-tuned model for generating job descriptions\n",
    "fine_tuned_model = GPT2LMHeadModel.from_pretrained(\"./fine-tuned-model\")\n",
    "\n",
    "# Now you can use `fine_tuned_model` to generate job descriptions based on job titles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experience & Qualifications:\n",
      "1. Please provide details of your relevant experience and qualifications:\n",
      "2. [SEP] Job Description Â  Send me Jobs like this Job Responsibilities: - Develop and maintain a strong team of technical and business leaders to support the business goals of the company. - Work closely with the Business Development team to ensure that the team is well-organized and well organized. Salary: Not Disclosed by Recruiter Industry: IT-Software / Software Services Functional Area: Sales, Retail, Business Intelligence Role Category:Retail Sales Role:Sales/Business Development Manager Keyskills Business development business development manager business intelligence business analysis business process Desired Candidate Profile Education- UG: Any Graduate PG:MBA/PGDM Doctorate:Any Doctoration - Any Specialization, Doctorates Not Required Please refer to the Job description above Company Profile: Confidential Confirm is a leading provider of IT services and solutions to clients in the fields of Banking, Finance, Insurance, Retail, IT, Telecom, Healthcare, Education,\n",
      "\n",
      "Hard Skills:\n",
      "1. List your hard skills and technical proficiencies: for sofware achitecht\n",
      "2. \n",
      "3. for sofaware analysing the data and analyzing the results. forsof warehousing achee acket achat aqua aptitude ase aq arya Desired Candidate Profile Education- UG: B.Tech/B.E. PG:Post Graduation Not Required Doctorate:Any Doctorates - Any Specialization, Doctoral Not required Please refer to the Job description above Company Profile: Confidential Confluence is a leading provider of IT services and solutions to Fortune 500 companies. We provide a wide range of services to our clients in the areas of Business Intelligence, IT Infrastructure Management, Business Process Outsourcing, and IT Software Development. Download PPT Photo 1 Â  View Contact Details\n",
      "4. [SEP] Job Description Â  Send me Jobs like this Job Title: IT Staffing Manager - IT Operations Job Location: Bangalore Job Type: Permanent/Contract\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the fine-tuned GPT-2 model\n",
    "fine_tuned_model = GPT2LMHeadModel.from_pretrained(\"./fine\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set the model to generate mode\n",
    "fine_tuned_model.eval()\n",
    "\n",
    "# Define a prompt for \"Experience & Qualifications\"\n",
    "experience_qualifications_prompt = \"Please provide details of your relevant experience and qualifications:\\n\"\n",
    "# Define a prompt for \"Hard Skills\"\n",
    "hard_skills_prompt = \"List your hard skills and technical proficiencies: for sofware achitecht\\n\"\n",
    "\n",
    "# Generate content for \"Experience & Qualifications\"\n",
    "generated_experience_qualifications = fine_tuned_model.generate(\n",
    "    input_ids=tokenizer.encode(experience_qualifications_prompt, return_tensors=\"pt\"),\n",
    "    max_length=200,  # Adjust the length as needed\n",
    "    num_return_sequences=1,  # Number of generated sequences\n",
    "    no_repeat_ngram_size=2,  # Avoid repeating phrases\n",
    "    top_k=50,  # Limit the selection to the top-k tokens\n",
    "    top_p=0.95,  # Limit the probability of tokens\n",
    "    temperature=0.7,  # Adjust the temperature for randomness\n",
    ")\n",
    "# Generate content for \"Hard Skills\"\n",
    "generated_hard_skills = fine_tuned_model.generate(\n",
    "    input_ids=tokenizer.encode(hard_skills_prompt, return_tensors=\"pt\"),\n",
    "    max_length=200,  # Adjust the length as needed\n",
    "    num_return_sequences=1,  # Number of generated sequences\n",
    "    no_repeat_ngram_size=2,  # Avoid repeating phrases\n",
    "    top_k=50,  # Limit the selection to the top-k tokens\n",
    "    top_p=0.95,  # Limit the probability of tokens\n",
    "    temperature=0.7,  # Adjust the temperature for randomness\n",
    ")\n",
    "\n",
    "# Decode the generated text for \"Experience & Qualifications\" and split it into points\n",
    "generated_experience_qualifications_text = tokenizer.decode(\n",
    "    generated_experience_qualifications[0], skip_special_tokens=True\n",
    ")\n",
    "experience_qualifications_points = generated_experience_qualifications_text.split(\"\\n\")\n",
    "\n",
    "# Decode the generated text for \"Hard Skills\" and split it into points\n",
    "generated_hard_skills_text = tokenizer.decode(\n",
    "    generated_hard_skills[0], skip_special_tokens=True\n",
    ")\n",
    "hard_skills_points = generated_hard_skills_text.split(\"\\n\")\n",
    "\n",
    "# Print the lists of points\n",
    "print(\"Experience & Qualifications:\")\n",
    "for i, point in enumerate(experience_qualifications_points):\n",
    "    print(f\"{i + 1}. {point}\")\n",
    "\n",
    "print(\"\\nHard Skills:\")\n",
    "for i, point in enumerate(hard_skills_points):\n",
    "    print(f\"{i + 1}. {point}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software Engineer Keyskills Java J2Ee JSP JMS JPA JEE JSF JAX-RS JQuery JUnit JBoss JIRA JBOSS JDeveloper JDE Desired Candidate Profile Education- UG: Any Graduate - Any Specialization PG:Any Postgraduate -Any Specializations Doctorate:Doctorate Not Required Please refer to the Job description above Company Profile: Confidential Confluence is a leading provider of IT services and solutions to Fortune 500 companies. We provide a wide range of services to our clients including: - IT Software - Application Programming, Maintenance Role Category:Programming & Design Role:Software Developer Keysky JAVA Jquery JSTL JSDLC JSRM JVMS Desirable Candidate Education - Ug: B.Tech/B.E. - Computers, BCA - Computer Science, MCA PG - Other Doctorates:M.Sc - Software Engineering, Computational Science Doctoral Doctor -(NonTechnical\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "def generate_resume_from_job_title(job_title, model_path=\"./fine\", max_length=200, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95, temperature=0.7):\n",
    "    # Load the fine-tuned GPT-2 model\n",
    "    fine_tuned_model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    \n",
    "    # Set the model to generate mode\n",
    "    fine_tuned_model.eval()\n",
    "\n",
    "    # Generate the resume content\n",
    "    generated_text = fine_tuned_model.generate(\n",
    "        input_ids=tokenizer.encode(job_title, return_tensors=\"pt\"),\n",
    "        max_length=max_length,  # Adjust the length as needed\n",
    "        num_return_sequences=num_return_sequences,  # Number of generated sequences\n",
    "        no_repeat_ngram_size=no_repeat_ngram_size,  # Avoid repeating phrases\n",
    "        top_k=top_k,  # Limit the selection to the top-k tokens\n",
    "        top_p=top_p,  # Limit the probability of tokens\n",
    "        temperature=temperature,  # Adjust the temperature for randomness\n",
    "    )\n",
    "\n",
    "    # Decode the generated text\n",
    "    generated_resume = tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_resume\n",
    "\n",
    "# Example usage:\n",
    "job_title = \"Software Engineer\"\n",
    "generated_resume = generate_resume_from_job_title(job_title)\n",
    "print(generated_resume)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
